# Medical Assistant with RAG

> A LLM(s)-powered medical assistant that answers med-related questions using **Retrieval-Augmented Generation (RAG)** over your curated PDF knowledge base.

[![Built with FastAPI](https://img.shields.io/badge/FastAPI-0.115+-green)](https://fastapi.tiangolo.com/)
[![ChromaDB](https://img.shields.io/badge/Vector%20Store-ChromaDB-blue)](https://www.trychroma.com/)
[![SentenceTransformers](https://img.shields.io/badge/Embeddings-all--MiniLM--L6--v2-purple)](https://www.sbert.net/)
[![HF Endpoint](https://img.shields.io/badge/HF--Endpoint-diyorarti%2Fmed--mixed--merged--qbi-yellow?logo=huggingface&logoColor=white)](https://huggingface.co/diyorarti/med-mixed-merged)
[![Dockerized](https://img.shields.io/badge/Docker-ready-informational)](https://www.docker.com/)
[![License](https://img.shields.io/badge/license-See%20LICENSE-lightgray)](#-license)

---

## âœ¨ Features

- **Production Ready RAG API** (FastAPI) with routes to 
      1.  **health** 
      2.  **index** 
      3.  **query** 
      4.  **upload** 
      5.  **delete** 
      6.  **stats**
- **Deterministic chunk IDs** and **stable metadata** for robust incremental indexing & deduplication.
- **Configurable chunking** (RecursiveCharacterTextSplitter + optional tiktoken length) with normalization/cleaning of PDF text.
- **Sentence-Transformers embeddings** (`all-MiniLM-L6-v2`) with optional normalization and batch encoding.
- **Persistent Vector Store** via **ChromaDB** (HNSW cosine) under `data/vector_store/`.
- **Two LLM providers**:
  - **Hugging Face Inference Endpoint** (default) via `langchain-huggingface`
  - **xAI Grok** via `langchain-xai` (optional)
- **Secure by default**: all (except health and stats) endpoints require `X-API-Key` header.
- **Docker-ready** image with healthcheck and `uvicorn` entrypoint.
- **Utilities & Labs**: caching demo (`data/cache`) and a development pipeline script under `rag/test/`.

---

## ğŸ§­ Project Overview

- **Status:** MVP / research-ready (version `0.1.0`)
- **Language/Stack:** Python 3.11, FastAPI, LangChain, SentenceTransformers, ChromaDB
- **LLMs:**
  - HF Endpoint (task: `text-generation`) â€“ configurable via `.env`
  - Grok (`grok-4`) â€“ optional, requires `GROK_API_KEY`
- **Embeddings:** `sentence-transformers/all-MiniLM-L6-v2` (configurable)
- **Persistence:** ChromaDB at `data/vector_store/` (collection-name: `pdf_documents`)
- **Data sources:** PDFs (used to build RAG) under `data/` (users upload PDFs are stored `data/uploads/`)
- **Security:** `X-API-Key` checked by dependency (`rag.core.security.verify_api_key`)
- **API Prefix:** `/v1`
- **Key Routes:** 
  - `GET /health` â€“ service health & active collection
  - `GET /v1/stats` â€“ collection size
  - `POST /v1/index` â€“ (re)index PDFs from `data/` (or a provided directory)
  - `POST /v1/upload` â€“ upload **PDFs** then auto-index
  - `POST /v1/query` â€“ RAG question answering (provider: `hf` or `grok`)
  - `DELETE /v1/delete` â€“ remove vectors by source file path

---

## ğŸ§± Architecture (High Level)

```bash
             â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   PDFs -->  â”‚ data/ (RAD-dev-pdfs)        â”‚
             â”‚ data/uploads (user-uploaded)â”‚ 
             â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                             â”‚  load_data() - (langchain_community.document_loaders -> PyPDFLoader)
                             â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚  CHUNKER      â”‚  â† cleans & splits PDF text (langchain -> RecursiveCharacterTextSplitter)
                      â”‚ (Recursive)   â”‚     (helpers.normalize_text)
                      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚  texts + metadata
                              â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚  EMBEDDER     â”‚  â† SentenceTransformers ("all-MiniLM-L6-v2" model )
                      â”‚               â”‚     (batch, normalized)
                      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜
                              â”‚  vectors + metadata (deterministic IDs)
                              â–¼
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚   ChromaDB     â”‚  â† persistent vector store
                      â”‚ (collection)   â”‚
                      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â–²
                       retrieve(top_k, threshold)
                              â”‚
                      â”Œâ”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                      â”‚   FastAPI       â”‚
                      â”‚   Routers:      â”‚
                      â”‚  /v1/index      â”‚   (build index from default PDFs)
                      â”‚  /v1/upload     â”‚   (upload PDFs + index)
   client question -> â”‚  /v1/query -----â”‚â”€â–º (LLM Providers: â€¢ HF Endpoint(default) (GROK optional))
                      â”‚  /v1/delete     â”‚   (Delete the indexed source)
                      â”‚  /health        â”‚   
                      â”‚  /stats         â”‚      
                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```
**Modules**
- `rag/api/` â€“ FastAPI app, routers, schemas, service layers.
- `rag/core/` â€“ configuration (config) API-key security.
- `rag/pipeline/` â€“ data-loader, chunker, embedder, retriever, vector-store,RAG pipelines(HF and GROK).
- `rag/pipeline/LLM` - grok-llm, hf-endpoint.
- `rag/utility/helpers.py` â€“ hashing, normalization,getting-chunk-id,text and meta extraftor ,ID generation, context formatting.
- `rag/test/` â€“ development pipeline with caching demo.
- `data/` â€“ PDFs, vector store persistence.
- `data/uploads/` - user uploaded pdfs.
- `data/cache` - chunks.pkl, embeddings.npy
- `labs/` â€“ development notebooks & lab `project-lab.ipynb` and  `requirements.txt`.

---

## ğŸ“ Project Structure
```bash
medical-assistant-with-rag/
â”‚
â”œâ”€â”€ .vscode/ # VSCode workspace settings
â”œâ”€â”€ data/ # Knowledge base and vector store
â”‚ â”œâ”€â”€ cache/ # Cache of chunks and embeddings
â”‚ â”‚ â”œâ”€â”€ chunks.pkl
â”‚ â”‚ â”œâ”€â”€ embeddings.npy
â”‚ â”‚ â””â”€â”€ manifest.json
â”‚ â”œâ”€â”€ uploads/ # User Uploaded PDFs for knowledge base
â”‚ â”‚ â””â”€â”€ 1706.03762v7.pdf
â”‚ â””â”€â”€ vector_store/ # ChromaDB persistence
â”‚ â”œâ”€â”€ chroma.sqlite3
â”‚ â”œâ”€â”€ Aging_natural_or_disease.pdf # RAG-dev-knowledge base
â”‚ â”œâ”€â”€ Genes_and_Disease.pdf # RAG-dev-knowledge base
â”‚ â””â”€â”€ basic_epidemiology.pdf # RAG-dev-knowledge base
â”‚
â”œâ”€â”€ hf-cache/ # Local Hugging Face model cache
â”‚
â”œâ”€â”€ labs/ # Research notebooks and experiments
â”‚ â”œâ”€â”€ project-lab.ipynb # RAG development lab
â”‚ â””â”€â”€ requirements.txt # packages used in LAB experiment
â”‚
â”œâ”€â”€ rag/ # Core application package
â”‚ â”œâ”€â”€ api/ # FastAPI endpoints, routers, and services
â”‚ â”‚ â”œâ”€â”€ routers/
â”‚ â”‚ â”œâ”€â”€ schemas/
â”‚ â”‚ â”œâ”€â”€ services/
â”‚ â”‚ â””â”€â”€ main.py # FastAPI entry point
â”‚ â”‚
â”‚ â”œâ”€â”€ core/ # App configuration and security
â”‚ â”‚ â”œâ”€â”€ config.py # Loads environment variables
â”‚ â”‚ â””â”€â”€ security.py # API key verification
â”‚ â”‚
â”‚ â”œâ”€â”€ pipeline/ # RAG pipeline components
â”‚ â”‚ â”œâ”€â”€ LLM/ # Large Language Model interfaces
â”‚ â”‚ â”œâ”€â”€ chunker.py # Text chunking logic
â”‚ â”‚ â”œâ”€â”€ data_loader.py # PDF loader and parser
â”‚ â”‚ â”œâ”€â”€ embedder.py # Embedding generation
â”‚ â”‚ â”œâ”€â”€ retriever.py # Retrieves relevant chunks from vector store
â”‚ â”‚ â”œâ”€â”€ vector_store.py # Handles ChromaDB operations
â”‚ â”‚ â”œâ”€â”€ hf_rag_pipeline.py # RAG pipeline using Hugging Face models
â”‚ â”‚ â””â”€â”€ grok_rag_pipeline.py # Optional RAG pipeline using Grok
â”‚ â”‚
â”‚ â”œâ”€â”€ test/ # Unit & dev-level tests
â”‚ â”‚ â””â”€â”€ rag_pipeline_dev.py
â”‚ â”‚
â”‚ â””â”€â”€ utility/ # Helper utilities
â”‚   â””â”€â”€ helpers.py # Text normalization, hashing, etc. 
â”‚
â”œâ”€â”€ .dockerignore
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .env # Environment variables (not for Git)
â”œâ”€â”€ Dockerfile # Docker setup
â”œâ”€â”€ LICENSE
â”œâ”€â”€ pyproject.toml # Project dependencies and metadata
â””â”€â”€ README.md # Project documentation
```


## âš™ï¸ Installation

### ğŸ§© Prerequisites
Before starting, make sure you have:

- **Python 3.10 or higher**
- **pip / venv** or **conda**
- *(optional)* **Docker 24+** if you prefer containerized deployment

---

### ğŸ—‚ï¸ Clone the Repository
```bash
git clone https://github.com/diyorarti/Medical-assistant-with-RAG.git
cd Medical-assistant-with-RAG

```bash
# Create venv
python -m venv .venv
# Activate (Linux/Mac)
source .venv/bin/activate
# Activate (Windows)
.venv\Scripts\activate
# installing packages
pip install -e .
```
or if Anaconda installed
```bash
# creating new virtual environment
conda create -n evnName python=3.10 -y
# activating created env
conda activate evnName
# installing packages
pip install -e .
```

### running project locally
```bash
uvicorn rag.api.main:app --reload
```
then Visit
â¡ï¸ Swagger UI: `http://127.0.0.1:8000/docs`
â¡ï¸ Healthcheck: `http://127.0.0.1:8000/health`

## ğŸ’» Usage & Examples
You can use the **Medical Assistant with RAG** in two ways:

1. **Through the REST API** (recommended for most users)
2. **As a Python module** (for developers building custom pipelines)
---

### ğŸŒ 1. Using the API

Once the FastAPI app is running (`uvicorn rag.api.main:app --reload`), open:

â¡ï¸ **Swagger UI:** [http://127.0.0.1:8000/docs](http://127.0.0.1:8000/docs)

There, you can test all endpoints interactively.

#### Example â€” Upload & Query via API

**Upload a PDF**
```bash
curl -X POST "http://127.0.0.1:8000/v1/upload" \
     -H "X-API-Key: your_api_key_here" \
     -F "file=@data/uploads/Aging_natural_or_disease.pdf"
```

### ğŸ§  2. Using the Python API
```bash
from rag.pipeline.hf_rag_pipeline import RAG_Simple_HF
from rag.pipeline.retriever import Retriever
from rag.pipeline.vector_store import VectorStore
from rag.pipeline.embedder import Embedder

embedder = Embedder()
vs = VectorStore()
retriever = Retriever(vs, embedder)

answer = RAG_Simple_HF("What causes neural degeneration?", retriever=retriever)
print(answer)

```

## â˜ï¸ Deployment
### ğŸ³ 2. Building and Run  Docker image
```bash
# buiding docker image
docker build -t medical-assistant-rag .
# running image
docker run --rm -it `
  --env-file .env `
  -p 8000:8000 `
  -v "${PWD}/data:/app/data" `
  -v "${PWD}/hf-cache:/root/.cache/huggingface" `
  --name medrag medrag-api:latest
```
then Visit:
â¡ï¸ Swagger UI: `http://127.0.0.1:8000/docs`
â¡ï¸ Healthcheck: `http://127.0.0.1:8000/health`

### ğŸš€ Deploy on Railway

1. Push your repo to GitHub.
2. Create a new project on Railway
3. Add environment variables in project settings.
4. Click Deploy.
5. Access your API from the live URL (e.g., https://medical-assistant.up.railway.app).

### ğŸ” Production Tips

Keep .env secrets private.
Use strong API_KEY.
Mount persistent volume for /data to retain embeddings.
Regularly back up data/vector_store.

### ğŸ“„ License
MIT License

### ğŸ™ Acknowledgements

[LangChain](https://www.langchain.com/)
[SentenceTransformers](https://www.sbert.net/)
[ChromaDB](https://www.trychroma.com/)
[FastAPI](https://fastapi.tiangolo.com/)
[Hugging Face](https://huggingface.co/)
[HF-Endpoint(LLM)](https://huggingface.co/diyorarti/med-mixed-merged)
[GROK(LLM)](https://x.ai/)