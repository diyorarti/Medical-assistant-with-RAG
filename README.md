# Medical Assistant with RAG

>[Med-assistant LLM](https://github.com/diyorarti/Medical-assistant) powered with RAG **Retrieval-Augmented Generation** that answers med-related questions using over your curated PDF knowledge base and defaut [PDFs](https://github.com/diyorarti/Medical-assistant-with-RAG/blob/main/labs/project-lab.ipynb).

[![Built with FastAPI](https://img.shields.io/badge/FastAPI-0.115+-green)](https://fastapi.tiangolo.com/)
[![ChromaDB](https://img.shields.io/badge/Vector%20Store-ChromaDB-blue)](https://www.trychroma.com/)
[![SentenceTransformers](https://img.shields.io/badge/Embeddings-all--MiniLM--L6--v2-purple)](https://www.sbert.net/)
[![HF Endpoint](https://img.shields.io/badge/HF--Endpoint-diyorarti%2Fmed--mixed--merged--qbi-yellow?logo=huggingface&logoColor=white)](https://huggingface.co/diyorarti/med-mixed-merged)
[![Dockerized](https://img.shields.io/badge/Docker-ready-informational)](https://www.docker.com/)
[![License](https://img.shields.io/badge/license-See%20LICENSE-lightgray)](#-license)

---
## üìñ Table of Contents
- [Features](#-features)
- [Project Overview](#-project-overview)
- [Tech Stack](#-tech-stack)
- [Architecture](#-architecture-high-level)
- [Project Structure](#-project-structure)
- [Installation](#Ô∏è-installation)
- [Usage & Examples](#-usage--examples)
- [Deployment](#Ô∏è-deployment)
- [Screenshots](#-screenshot)
- [License & Acknowledgements](#-license)

---
## ‚ú® Features

- **Context-Aware Q&A:** Retrieves relevant medical information from uploaded PDFs and default [knowledge base](https://github.com/diyorarti/Medical-assistant-with-RAG/blob/main/labs/project-lab.ipynb) before generating answers.  
- **Retrieval-Augmented Generation (RAG):** Combines vector-based document search (ChromaDB) with an [Fine-tuned LLM](https://huggingface.co/diyorarti/med-mixed-merged) for accurate, reference-supported responses.  
- **Multi-Source Document Support:** Handles multiple uploaded medical documents, automatically chunked and embedded for efficient retrieval.  
- **Fast and Scalable API:** Built with FastAPI and Docker for seamless deployment and real-time interaction.  
- **Custom Embedding Pipeline:** Uses SentenceTransformers (`all-MiniLM-L6-v2`) for precise vector representation of text.  
- **Swagger UI Documentation:** Provides interactive API exploration for developers.  
---

## ü©∫ Project Overview

In healthcare and medical research, professionals often face challenges in finding precise and trustworthy information buried within lengthy documents. Manual searching not only wastes time but increases the risk of overlooking critical insights.

**Medical Assistant with RAG** provides an intelligent solution by combining *retrieval-augmented generation (RAG)* and *large language models (LLMs)*. Users can upload medical PDFs, ask natural-language questions, and receive accurate, context-rich answers grounded in the uploaded sources.

Technically, the system uses **LangChain**, **ChromaDB**, and **SentenceTransformers (all-MiniLM-L6-v2)** for vector-based retrieval, and connects to **Hugging Face** or **Grok (xAI)** APIs for language generation. The backend is implemented with **FastAPI**, containerized with **Docker**, and deployed via **Render**, providing real-time, scalable access.

---

## ‚öôÔ∏è Tech Stack

| Area | Technologies |
|------|---------------|
| **Programming Language** | Python |
| **Framework** | FastAPI |
| **RAG Components** | LangChain, SentenceTransformers, ChromaDB |
| **Embedding Model** | all-MiniLM-L6-v2 |
| **LLM Providers** | Hugging Face Endpoint, Grok(optional) |
| **Vector Database** | ChromaDB |
| **Document Processing** | PyPDFLoader, RecursiveCharacterTextSplitter |
| **Containerization** | Docker |
| **Deployment** | Render |
---

## üß± Architecture (High-Level)
The project implements a Retrieval-Augmented Generation (RAG) pipeline that combines local document retrieval with remote LLM inference.  
Below is the high-level data and control flow within the system:
```bash
             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   PDFs -->  ‚îÇ data/ (RAD-dev-pdfs)        ‚îÇ
             ‚îÇ data/uploads (user-uploaded)‚îÇ 
             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ  load_data() - (langchain_community.document_loaders -> PyPDFLoader)
                             ‚ñº
                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                      ‚îÇ  CHUNKER      ‚îÇ  ‚Üê cleans & splits PDF text (langchain -> RecursiveCharacterTextSplitter)
                      ‚îÇ (Recursive)   ‚îÇ     (helpers.normalize_text)
                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ  texts + metadata
                              ‚ñº
                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                      ‚îÇ  EMBEDDER     ‚îÇ  ‚Üê SentenceTransformers ("all-MiniLM-L6-v2" model )
                      ‚îÇ               ‚îÇ     (batch, normalized)
                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ  vectors + metadata (deterministic IDs)
                              ‚ñº
                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                      ‚îÇ   ChromaDB     ‚îÇ  ‚Üê persistent vector store
                      ‚îÇ (collection)   ‚îÇ
                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚ñ≤
                       retrieve(top_k, threshold)
                              ‚îÇ
                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                      ‚îÇ   FastAPI       ‚îÇ
                      ‚îÇ   Routers:      ‚îÇ
                      ‚îÇ  /v1/index      ‚îÇ   (build index from default PDFs)
                      ‚îÇ  /v1/upload     ‚îÇ   (upload PDFs + index)
   client question -> ‚îÇ  /v1/query      ‚îÇ   (LLM Providers: ‚Ä¢ HF Endpoint(default) (GROK optional))
                      ‚îÇ  /v1/delete     ‚îÇ   (Delete the indexed source)
                      |  /root          |
                      ‚îÇ  /health        ‚îÇ   
                      ‚îÇ  /v1/stats      ‚îÇ  
                      |  /v1/debug/ls   |     
                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```
---
## üìÅ Project Structure
```bash
medical-assistant-with-rag/
‚îÇ
‚îú‚îÄ‚îÄ .vscode/ # VSCode workspace settings
‚îú‚îÄ‚îÄ assets/ # project related pictures
‚îú‚îÄ‚îÄ data/ # Knowledge base and vector store
‚îÇ ‚îú‚îÄ‚îÄ cache/ # Cache of chunks and embeddings
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ chunks.pkl
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ embeddings.npy
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ manifest.json
‚îÇ ‚îú‚îÄ‚îÄ uploads/ # User Uploaded PDFs for knowledge base
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 1706.03762v7.pdf # example of user uploaded file 
‚îÇ ‚îî‚îÄ‚îÄ vector_store/ # ChromaDB persistence
‚îÇ ‚îú‚îÄ‚îÄ chroma.sqlite3
‚îÇ ‚îú‚îÄ‚îÄ Aging_natural_or_disease.pdf # RAG-dev-knowledge base
‚îÇ ‚îú‚îÄ‚îÄ Genes_and_Disease.pdf # RAG-dev-knowledge base
‚îÇ ‚îî‚îÄ‚îÄ basic_epidemiology.pdf # RAG-dev-knowledge base
‚îÇ
‚îú‚îÄ‚îÄ hf-cache/ # Local Hugging Face model cache
‚îÇ
‚îú‚îÄ‚îÄ labs/ # Research notebooks and experiments
‚îÇ ‚îú‚îÄ‚îÄ project-lab.ipynb # RAG development lab
‚îÇ ‚îî‚îÄ‚îÄ requirements.txt # packages used in LAB experiment
‚îÇ
‚îú‚îÄ‚îÄ rag/ # Core application package
‚îÇ ‚îú‚îÄ‚îÄ api/ # FastAPI endpoints, routers, and services
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ routers/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ schemas/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ services/
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ main.py # FastAPI entry point
‚îÇ ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ core/ # App configuration and security
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ config.py # Loads environment variables
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ security.py # API key verification
‚îÇ ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ pipeline/ # RAG pipeline components
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ LLM/ # Large Language Model interfaces
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ chunker.py # Text chunking logic
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ data_loader.py # PDF loader and parser
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ embedder.py # Embedding generation
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ retriever.py # Retrieves relevant chunks from vector store
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ vector_store.py # Handles ChromaDB operations
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ hf_rag_pipeline.py # RAG pipeline using Hugging Face models
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ grok_rag_pipeline.py # Optional RAG pipeline using Grok
‚îÇ ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ test/ # Unit & dev-level tests
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ rag_pipeline_dev.py
‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ utility/ # Helper utilities
‚îÇ   ‚îî‚îÄ‚îÄ helpers.py # Text normalization, hashing, etc. 
‚îÇ
‚îú‚îÄ‚îÄ .dockerignore
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .env # Environment variables (not for Git)
‚îú‚îÄ‚îÄ Dockerfile # Docker setup
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ pyproject.toml # Project dependencies and metadata
‚îî‚îÄ‚îÄ README.md # Project documentation
```


## ‚öôÔ∏è Installation

### üß© Prerequisites
Before you begin, ensure you have the following installed:

- **Python 3.10+**
- **pip** or **conda** for package management
- *(Optional)* **Docker 24+** if you prefer containerized deployment

---

### üóÇÔ∏è Clone the Repository

```bash
git clone https://github.com/diyorarti/Medical-assistant-with-RAG.git
cd Medical-assistant-with-RAG
```

### running project locally
```bash
uvicorn rag.api.main:app --reload
```

## üíª Usage 

## ‚òÅÔ∏è Deployment
### üê≥ 2. Building and Run  Docker image
```bash
# buiding docker image
docker build -t medical-assistant-rag .
# running image
docker run --rm -it `
  --env-file .env `
  -p 8000:8000 `
  -v "${PWD}/data:/app/data" `
  -v "${PWD}/hf-cache:/root/.cache/huggingface" `
  --name medrag medrag-api:latest
```

### üöÄ Deploy on Render
Project deployed on [Render](https://medical-assistant-with-rag.onrender.com/docs)
Note: Once I deployed the project on Render successfully,then I stopped the paid subscription version of Render due to finiancial reasons, now it does not work, because min 2+ Ram and 5+ memory are required to run this project.
### ‚öôÔ∏è Steps to Deploy on Render
**1. Create a new Web Service on Render:**

Go to https://render.com
Click ‚ÄúNew +‚Äù ‚Üí ‚ÄúWeb Service‚Äù
Connect your GitHub repo
Select your repo ‚Üí click Connect
**2. Render build settings:**

Environment ->	Docker
Region	Closest to you
Instance Type	‚úÖ Standard (2 GB RAM) (avoid free tier for embeddings)
**3. Attach a Persistent Disk**

In the service ‚Üí Settings ‚Üí Disks ‚Üí Add Disk
Name: storage
Mount Path: /app/storage
Size: e.g. min 5 GB
This disk stores:
PDFs (DATA_DIR)
Vector store (Chroma)
Hugging Face cache
**4. Add environment variables**

Go to Settings ‚Üí Environment ‚Üí Add Environment Variable
| Key |	Value |
|------|--------------------|
| HF_TOKEN | HuggingFace access token |
| GROK_API_KEY | XAI api key |
| DATA_DIR |	/app/storage |
| PERSIST_DIRECTORY_VS |	/app/storage/vector_store |
| HF_HOME |	/app/storage/hf-cache |
| HUGGINGFACE_HUB_CACHE |	/app/storage/hf-cache |
| API_KEY |	(your secret key ‚Äî used in verify_api_key) |


## üì∏ Screenshot
 ### ALL APIs
![Swagger UI Screenshot](assets/api.png)
when I got the picture, the debug endpoint was running, then I removed it . 
---
### stats endpoint
![Swagger UI Screenshot](assets/stats-endpoint.png)
---
### index endpoint
![Swagger UI Screenshot](assets/index-endpoint.png)
---
### upload endpoint 
![Swagger UI Screenshot](assets/upload-ednpoint.png)
---
### query endpoint 
![Swagger UI Screenshot](assets/query-endpoint.png)
---
### delete endpoint 
![Swagger UI Screenshot](assets/delete-endpoint.png)
---

### üìÑ License
MIT License

### üôè Acknowledgements

[LangChain](https://www.langchain.com/)                                                                             
[SentenceTransformers](https://www.sbert.net/)  
[ChromaDB](https://www.trychroma.com/)                                
[FastAPI](https://fastapi.tiangolo.com/)                                                  
[Hugging Face](https://huggingface.co/)                                                 
[HF-Endpoint(LLM)](https://huggingface.co/diyorarti/med-mixed-merged)
[Med-assistant-with-RAG-API](https://medical-assistant-with-rag.onrender.com/docs)                                               
[GROK(LLM)](https://x.ai/)                                                  