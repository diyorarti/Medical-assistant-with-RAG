# Medical Assistant with RAG

>[Med-assistant LLM](https://github.com/diyorarti/Medical-assistant) powered with RAG **Retrieval-Augmented Generation** that answers med-related questions using over your curated PDF knowledge base and defaut [PDFs](https://github.com/diyorarti/Medical-assistant-with-RAG/blob/main/labs/project-lab.ipynb).

[![Built with FastAPI](https://img.shields.io/badge/FastAPI-0.115+-green)](https://fastapi.tiangolo.com/)
[![ChromaDB](https://img.shields.io/badge/Vector%20Store-ChromaDB-blue)](https://www.trychroma.com/)
[![SentenceTransformers](https://img.shields.io/badge/Embeddings-all--MiniLM--L6--v2-purple)](https://www.sbert.net/)
[![HF Endpoint](https://img.shields.io/badge/HF--Endpoint-diyorarti%2Fmed--mixed--merged--qbi-yellow?logo=huggingface&logoColor=white)](https://huggingface.co/diyorarti/med-mixed-merged)
[![Dockerized](https://img.shields.io/badge/Docker-ready-informational)](https://www.docker.com/)
[![License](https://img.shields.io/badge/license-See%20LICENSE-lightgray)](#-license)

---

## ‚ú® Features
- [**Production Ready RAG API** (FastAPI)](https://medical-assistant-with-rag.onrender.com/docs)                  
- **Deterministic chunk IDs** and **stable metadata** for robust incremental indexing & deduplication.
- **Configurable chunking** (RecursiveCharacterTextSplitter + optional tiktoken length) with normalization/cleaning of PDF text.
- **Sentence-Transformers embeddings** (`all-MiniLM-L6-v2`) with optional normalization and batch encoding.
- **Persistent Vector Store** via **ChromaDB** under `data/vector_store/`.
- **Two LLM providers**:
  - [**Hugging Face Inference Endpoint**](https://huggingface.co/diyorarti/med-mixed-merged) deployed in HF inference endpoints (default LLM)
  - **xAI Grok** (optional)
- **Secure by default**: endpoints (index, upload, query and delete) require `X-API-Key` header.
- **Docker-ready** image with healthcheck and `uvicorn` entrypoint.
- **Utilities & Labs**: caching demo (`data/cache`) and a development pipeline script under `rag/test/`.

---

## üß≠ Project Overview

- **Language/Stack:** Python 3.11, FastAPI, LangChain, SentenceTransformers, ChromaDB
- **LLMs:**
  - HF Endpoint (task: `text-generation`) ‚Äì configurable via `.env`
  - Grok (`grok-4`) ‚Äì optional, requires `GROK_API_KEY`
- **Embeddings:** `sentence-transformers/all-MiniLM-L6-v2` (configurable)
- **Persistence:** ChromaDB at `data/vector_store/` (collection-name: `pdf_documents`)
- **Data sources:** PDFs (used to build RAG) under `data/` (users upload PDFs are stored `data/uploads/`)
- **Security:** `X-API-Key` checked by dependency (`rag.core.security.verify_api_key`)
- **API Prefix:** `/v1`
- **Key Routes:**
  - `GET /health` ‚Äì service health & active collection
  - `GET /v1/stats` ‚Äì collection size
  - `POST /v1/index` ‚Äì (re)index PDFs from `data/` (or a provided directory)
  - `POST /v1/upload` ‚Äì upload **PDFs** then auto-index
  - `POST /v1/query` ‚Äì RAG question answering (provider: `hf` or `grok`)
  - `DELETE /v1/delete` ‚Äì remove vectors by source file path

---

## üß© Tech Stack

| Area | Tools / Libraries |
|------|--------------------|
| Programming Language | Python |
| Framework | FastAPI |
| RAG pipeline Compnents | Langchain, SentenceTransformers, ChromaDB |
| LLM Providers | HF-Endpoint, GROK |
| EMbedding-Model | sentence-transformers/all-MiniLM-L6-v2 |
| Vector DataBase | ChromabDB |
| Document Processing | Lanchain-> PyPDFLoader, RecursiveCharacterTextSplitter
| Containerization | Docker |
| Deployment | Render |

---


## üß± Architecture (High Level)

```bash
             ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
   PDFs -->  ‚îÇ data/ (RAD-dev-pdfs)        ‚îÇ
             ‚îÇ data/uploads (user-uploaded)‚îÇ 
             ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                             ‚îÇ  load_data() - (langchain_community.document_loaders -> PyPDFLoader)
                             ‚ñº
                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                      ‚îÇ  CHUNKER      ‚îÇ  ‚Üê cleans & splits PDF text (langchain -> RecursiveCharacterTextSplitter)
                      ‚îÇ (Recursive)   ‚îÇ     (helpers.normalize_text)
                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ  texts + metadata
                              ‚ñº
                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                      ‚îÇ  EMBEDDER     ‚îÇ  ‚Üê SentenceTransformers ("all-MiniLM-L6-v2" model )
                      ‚îÇ               ‚îÇ     (batch, normalized)
                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚îÇ  vectors + metadata (deterministic IDs)
                              ‚ñº
                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                      ‚îÇ   ChromaDB     ‚îÇ  ‚Üê persistent vector store
                      ‚îÇ (collection)   ‚îÇ
                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                              ‚ñ≤
                       retrieve(top_k, threshold)
                              ‚îÇ
                      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                      ‚îÇ   FastAPI       ‚îÇ
                      ‚îÇ   Routers:      ‚îÇ
                      ‚îÇ  /v1/index      ‚îÇ   (build index from default PDFs)
                      ‚îÇ  /v1/upload     ‚îÇ   (upload PDFs + index)
   client question -> ‚îÇ  /v1/query -----‚îÇ‚îÄ‚ñ∫ (LLM Providers: ‚Ä¢ HF Endpoint(default) (GROK optional))
                      ‚îÇ  /v1/delete     ‚îÇ   (Delete the indexed source)
                      ‚îÇ  /health        ‚îÇ   
                      ‚îÇ  /stats         ‚îÇ      
                      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```
**Modules**
- `rag/api/` ‚Äì FastAPI app, routers, schemas, service layers.
- `rag/core/` ‚Äì configuration (config) API-key security.
- `rag/pipeline/` ‚Äì data-loader, chunker, embedder, retriever, vector-store,RAG pipelines(HF and GROK).
- `rag/pipeline/LLM` - grok-llm, hf-endpoint.
- `rag/utility/helpers.py` ‚Äì hashing, normalization,getting-chunk-id,text and meta extraftor ,ID generation, context formatting.
- `rag/test/` ‚Äì development pipeline with caching demo.
- `data/` ‚Äì PDFs, vector store persistence.
- `data/uploads/` - user uploaded pdfs.
- `data/cache` - chunks.pkl, embeddings.npy
- `labs/` ‚Äì development notebooks & lab `project-lab.ipynb` and  `requirements.txt`.

---

## üìÅ Project Structure
```bash
medical-assistant-with-rag/
‚îÇ
‚îú‚îÄ‚îÄ .vscode/ # VSCode workspace settings
‚îú‚îÄ‚îÄ data/ # Knowledge base and vector store
‚îÇ ‚îú‚îÄ‚îÄ cache/ # Cache of chunks and embeddings
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ chunks.pkl
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ embeddings.npy
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ manifest.json
‚îÇ ‚îú‚îÄ‚îÄ uploads/ # User Uploaded PDFs for knowledge base
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ 1706.03762v7.pdf
‚îÇ ‚îî‚îÄ‚îÄ vector_store/ # ChromaDB persistence
‚îÇ ‚îú‚îÄ‚îÄ chroma.sqlite3
‚îÇ ‚îú‚îÄ‚îÄ Aging_natural_or_disease.pdf # RAG-dev-knowledge base
‚îÇ ‚îú‚îÄ‚îÄ Genes_and_Disease.pdf # RAG-dev-knowledge base
‚îÇ ‚îî‚îÄ‚îÄ basic_epidemiology.pdf # RAG-dev-knowledge base
‚îÇ
‚îú‚îÄ‚îÄ hf-cache/ # Local Hugging Face model cache
‚îÇ
‚îú‚îÄ‚îÄ labs/ # Research notebooks and experiments
‚îÇ ‚îú‚îÄ‚îÄ project-lab.ipynb # RAG development lab
‚îÇ ‚îî‚îÄ‚îÄ requirements.txt # packages used in LAB experiment
‚îÇ
‚îú‚îÄ‚îÄ rag/ # Core application package
‚îÇ ‚îú‚îÄ‚îÄ api/ # FastAPI endpoints, routers, and services
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ routers/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ schemas/
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ services/
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ main.py # FastAPI entry point
‚îÇ ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ core/ # App configuration and security
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ config.py # Loads environment variables
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ security.py # API key verification
‚îÇ ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ pipeline/ # RAG pipeline components
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ LLM/ # Large Language Model interfaces
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ chunker.py # Text chunking logic
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ data_loader.py # PDF loader and parser
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ embedder.py # Embedding generation
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ retriever.py # Retrieves relevant chunks from vector store
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ vector_store.py # Handles ChromaDB operations
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ hf_rag_pipeline.py # RAG pipeline using Hugging Face models
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ grok_rag_pipeline.py # Optional RAG pipeline using Grok
‚îÇ ‚îÇ
‚îÇ ‚îú‚îÄ‚îÄ test/ # Unit & dev-level tests
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ rag_pipeline_dev.py
‚îÇ ‚îÇ
‚îÇ ‚îî‚îÄ‚îÄ utility/ # Helper utilities
‚îÇ   ‚îî‚îÄ‚îÄ helpers.py # Text normalization, hashing, etc. 
‚îÇ
‚îú‚îÄ‚îÄ .dockerignore
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ .env # Environment variables (not for Git)
‚îú‚îÄ‚îÄ Dockerfile # Docker setup
‚îú‚îÄ‚îÄ LICENSE
‚îú‚îÄ‚îÄ pyproject.toml # Project dependencies and metadata
‚îî‚îÄ‚îÄ README.md # Project documentation
```


## ‚öôÔ∏è Installation

### üß© Prerequisites
Before starting, make sure you have:

- **Python 3.10 or higher**
- **pip / venv** or **conda**
- *(optional)* **Docker 24+** if you prefer containerized deployment

---

### üóÇÔ∏è Clone the Repository
```bash
git clone https://github.com/diyorarti/Medical-assistant-with-RAG.git
cd Medical-assistant-with-RAG

```bash
# Create venv
python -m venv .venv
# Activate (Linux/Mac)
source .venv/bin/activate
# Activate (Windows)
.venv\Scripts\activate
# installing packages
pip install -e .
```
or if Anaconda installed
```bash
# creating new virtual environment
conda create -n evnName python=3.10 -y
# activating created env
conda activate evnName
# installing packages
pip install -e .
```

### running project locally
```bash
uvicorn rag.api.main:app --reload
```
## üìò Swagger Documentation:
then Visit
‚û°Ô∏è Swagger UI: `http://127.0.0.1:8000/docs`

## üíª Usage & Examples
You can use the **Medical Assistant with RAG** in two ways:

1. **Through the REST API** (recommended for most users)
2. **As a Python module** (for developers building custom pipelines)
---

### üåê 1. Using the API

Once the FastAPI app is running (`uvicorn rag.api.main:app --reload`), open:

‚û°Ô∏è [**Swagger UI:**](http://127.0.0.1:8000/docs) 

There, you can test all endpoints interactively.

#### Example ‚Äî Upload & Query via API

**Upload a PDF**
```bash
curl -X POST "http://127.0.0.1:8000/v1/upload" \
     -H "X-API-Key: your_api_key_here" \
     -F "file=@data/uploads/Aging_natural_or_disease.pdf"
```

### üß† 2. Using the Python API
```bash
from rag.pipeline.hf_rag_pipeline import RAG_Simple_HF
from rag.pipeline.retriever import Retriever
from rag.pipeline.vector_store import VectorStore
from rag.pipeline.embedder import Embedder

embedder = Embedder()
vs = VectorStore()
retriever = Retriever(vs, embedder)

answer = RAG_Simple_HF("What causes neural degeneration?", retriever=retriever)
print(answer)

```

## ‚òÅÔ∏è Deployment
### üê≥ 2. Building and Run  Docker image
```bash
# buiding docker image
docker build -t medical-assistant-rag .
# running image
docker run --rm -it `
  --env-file .env `
  -p 8000:8000 `
  -v "${PWD}/data:/app/data" `
  -v "${PWD}/hf-cache:/root/.cache/huggingface" `
  --name medrag medrag-api:latest
```
then Visit:
‚û°Ô∏è Swagger UI: `http://127.0.0.1:8000/docs`

### üöÄ Deploy 
Project deployed on [Render](https://medical-assistant-with-rag.onrender.com/docs)
Note: Once I deployed the project on Render successfully,then I stopped the paid subscription version of Render due to finiancial reasons, now it does not work, because min 2+ Ram and 5+ memory are required to run this project.

## üì∏ Screenshot
 ### ALL APIs
![Swagger UI Screenshot](assets/api.png)
when I got the picture, the debug endpoint was running, then I removed it . 
---
### stats endpoint
![Swagger UI Screenshot](assets/stats-endpoint.png)
---
### index endpoint
![Swagger UI Screenshot](assets/index-endpoint.png)
---
### upload endpoint 
![Swagger UI Screenshot](assets/upload-ednpoint.png)
---
### query endpoint 
![Swagger UI Screenshot](assets/query-endpoint.png)
---
### delete endpoint 
![Swagger UI Screenshot](assets/delete-endpoint.png)
---

## Deployment steps on Render
### 1. Clone the repo: 
```bash
git clone https://github.com/diyorarti/Medical-assistant-with-RAG.git
```
### 2. Create a new Web Service on Render:
Go to https://render.com
Click ‚ÄúNew +‚Äù ‚Üí ‚ÄúWeb Service‚Äù
Connect your GitHub repo
Select your repo ‚Üí click Connect
### 3. Render build settings:
Environment ->	Docker
Region	Closest to you
Instance Type	‚úÖ Standard (2 GB RAM) (avoid free tier for embeddings)
### 4. Attach a Persistent Disk
In the service ‚Üí Settings ‚Üí Disks ‚Üí Add Disk
Name: storage
Mount Path: /app/storage
Size: e.g. 5 GB
This disk stores:
PDFs (DATA_DIR)
Vector store (Chroma/FAISS)
Hugging Face cache
### 5. Add environment variables
Go to Settings ‚Üí Environment ‚Üí Add Environment Variable

| Key |	Value |
|------|--------------------|
| HF_TOKEN | HuggingFace access token |
| GROK_API_KEY | XAI api key |
| DATA_DIR |	/app/storage |
| PERSIST_DIRECTORY_VS |	/app/storage/vector_store |
| HF_HOME |	/app/storage/hf-cache |
| HUGGINGFACE_HUB_CACHE |	/app/storage/hf-cache |
| API_KEY |	(your secret key ‚Äî used in verify_api_key) |

### üìÑ License
MIT License

### üôè Acknowledgements

[LangChain](https://www.langchain.com/)                                                                             
[SentenceTransformers](https://www.sbert.net/)  
[ChromaDB](https://www.trychroma.com/)                                
[FastAPI](https://fastapi.tiangolo.com/)                                                  
[Hugging Face](https://huggingface.co/)                                                 
[HF-Endpoint(LLM)](https://huggingface.co/diyorarti/med-mixed-merged)
[Med-assistant-with-RAG-API](https://medical-assistant-with-rag.onrender.com/docs)                                               
[GROK(LLM)](https://x.ai/)                                                  